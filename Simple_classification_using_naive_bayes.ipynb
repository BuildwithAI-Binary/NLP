{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'final_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import operator\n",
    "import string\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "import re\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Text':'text','Mental_condition':'label'},inplace=True)\n",
    "df['length'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "different preprocess functions\n",
    "'''\n",
    "def preprocess(Text):\n",
    "    Text = Text.str.replace(\"(<br/>)\", \"\")\n",
    "    Text = Text.str.replace('(<a).*(>).*(</a>)', '')\n",
    "    Text = Text.str.replace('(&amp)', '')\n",
    "    Text = Text.str.replace('(&gt)', '')\n",
    "    Text = Text.str.replace('(&lt)', '')\n",
    "    Text = Text.str.replace('(\\xa0…)', ' ')  \n",
    "    Text = Text.str.replace('(\\n)', ' ')\n",
    "    Text = Text.str.replace('+', '')\n",
    "    return Text\n",
    "\n",
    "# remove urls from text\n",
    "def remove_urls(row):\n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    row =  url_pattern.sub(r'', row)\n",
    "    return row\n",
    "\n",
    "# remove @accounts (@[A-Za-z0–9]+)|, emojis，#hashtags, com frin text\n",
    "def removes(row):\n",
    "    url_pattern = re.compile(r\"(\\w+:\\/\\/\\S+)|(#(\\w+))|(.twitter.com/[A-Za-z0–9]+)\")\n",
    "    row =  url_pattern.sub(r'', row)\n",
    "    return row\n",
    "\n",
    "def removes(row):\n",
    "    url_pattern = re.compile(r\"(\\w+:\\/\\/\\S+)|(#(\\w+))|(.twitter.com/[A-Za-z0–9]+)\")\n",
    "    row =  url_pattern.sub(r'', row)\n",
    "    return row\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)   \n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply those functions\n",
    "df['text'] = preprocess(df.text)\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "df['text'] = df['text'].apply(removes)\n",
    "df.text = df.text.apply(lambda x: decontracted(x).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "not_stopwords = {}\n",
    "common_words =['com','twitter','please',\n",
    "               'the','would','could','can','may','must', \n",
    "              # 'us','i','you','this','we','me','that','it',\n",
    "               'de','s','pic','t','m','en','la','que','don','re','e','el','ca']\n",
    "stop_words.extend(word for word in common_words if word not in stop_words)\n",
    "final_stop_words = set([word for word in stop_words if word not in not_stopwords])\n",
    "df.text = df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in final_stop_words]))\n",
    "rm_punc = re.compile(r\"(\\W)\")\n",
    "rm_space = re.compile(r\"(\\s+)\")\n",
    "df.text = df.text.apply(lambda x: rm_punc.sub(' ', x))\n",
    "df.text = df.text.apply(lambda x: rm_space.sub(' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(df.label.unique())\n",
    "text = list(df.text.values)\n",
    "X = [create_vector(i) for i in text]\n",
    "Y = list(df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as TF\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(df.text, df.label, random_state = 42, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CV()),\n",
    "               ('clf', MB()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['pos','dep','sucide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "y_pred = nb.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
